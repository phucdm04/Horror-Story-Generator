{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\dangm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dangm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2023.12.25)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dangm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dangm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~abel-studio (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~abel-studio (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~abel-studio (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dangm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4 regex\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re   \n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 1. Done!\n",
      "Fetching data from page 2. Done!\n",
      "Fetching data from page 3. Done!\n",
      "Fetching data from page 4. Done!\n",
      "Fetching data from page 5. Error: list index out of range at St. Patrickâ€™s Night\n",
      "Done!\n",
      "Fetching data from page 6. Done!\n",
      "Fetching data from page 7. Done!\n",
      "Fetching data from page 8. Done!\n",
      "Fetching data from page 9. Done!\n",
      "Fetching data from page 10. Done!\n",
      "Fetching data from page 11. Done!\n",
      "Fetching data from page 12. Done!\n",
      "Fetching data from page 13. Done!\n",
      "Fetching data from page 14. Find nothing!\n",
      "Done!\n",
      "Fetching data from page 15. Find nothing!\n",
      "Done!\n",
      "Fetching data from page 16. Done!\n",
      "Fetching data from page 17. Done!\n",
      "Fetching data from page 18. Done!\n",
      "Fetching data from page 19. Done!\n",
      "Fetching data from page 20. Done!\n",
      "Fetching data from page 21. Done!\n",
      "Fetching data from page 22. Done!\n",
      "Fetching data from page 23. Done!\n",
      "Fetching data from page 24. Done!\n",
      "Fetching data from page 25. Done!\n",
      "Fetching data from page 26. Done!\n",
      "Fetching data from page 27. Done!\n",
      "Fetching data from page 28. Done!\n",
      "Fetching data from page 29. Done!\n",
      "Fetching data from page 30. Done!\n",
      "Fetching data from page 31. Done!\n",
      "Fetching data from page 32. Done!\n",
      "Fetching data from page 33. Done!\n",
      "Fetching data from page 34. Done!\n",
      "Fetching data from page 35. Done!\n",
      "Fetching data from page 36. Done!\n",
      "Fetching data from page 37. Done!\n",
      "Fetching data from page 38. Done!\n",
      "Fetching data from page 39. Done!\n",
      "Fetching data from page 40. Done!\n",
      "Fetching data from page 41. Done!\n",
      "Fetching data from page 42. Done!\n",
      "Fetching data from page 43. Done!\n",
      "Fetching data from page 44. Done!\n",
      "Fetching data from page 45. Done!\n",
      "Fetching data from page 46. Done!\n",
      "Fetching data from page 47. Done!\n",
      "Fetching data from page 48. Done!\n",
      "Fetching data from page 49. Done!\n",
      "Fetching data from page 50. Done!\n",
      "Fetching data from page 51. Done!\n",
      "Fetching data from page 52. Done!\n",
      "Fetching data from page 53. Done!\n",
      "Fetching data from page 54. Done!\n",
      "Fetching data from page 55. Done!\n",
      "Fetching data from page 56. Done!\n",
      "Fetching data from page 57. Done!\n",
      "Fetching data from page 58. Done!\n",
      "Fetching data from page 59. Done!\n",
      "Fetching data from page 60. Done!\n",
      "Fetching data from page 61. Done!\n",
      "Fetching data from page 62. Done!\n",
      "Fetching data from page 63. Done!\n",
      "Fetching data from page 64. Done!\n",
      "Fetching data from page 65. Done!\n",
      "Fetching data from page 66. Done!\n",
      "Fetching data from page 67. Done!\n",
      "Fetching data from page 68. Done!\n",
      "Fetching data from page 69. Done!\n",
      "Fetching data from page 70. Done!\n",
      "Fetching data from page 71. Done!\n",
      "Fetching data from page 72. Done!\n",
      "Fetching data from page 73. Done!\n",
      "Fetching data from page 74. Done!\n",
      "Fetching data from page 75. Done!\n",
      "Fetching data from page 76. Done!\n",
      "Fetching data from page 77. Done!\n",
      "Fetching data from page 78. Done!\n",
      "Fetching data from page 79. Done!\n",
      "Fetching data from page 80. Done!\n",
      "Fetching data from page 81. Done!\n",
      "Fetching data from page 82. Done!\n",
      "Fetching data from page 83. Done!\n",
      "Fetching data from page 84. Done!\n",
      "Fetching data from page 85. Done!\n",
      "Fetching data from page 86. Done!\n",
      "Fetching data from page 87. Done!\n",
      "Fetching data from page 88. Done!\n",
      "Fetching data from page 89. No post found!\n"
     ]
    }
   ],
   "source": [
    "def fetch_one_page(url: str) -> dict:\n",
    "    def get_rating_and_votes(text: str) -> tuple:\n",
    "        match = re.search(r'Rating: (\\d+\\.\\d+)/10\\. From (\\d+) vote[s]?\\.', text)\n",
    "        if match:\n",
    "            rating = match.group(1)\n",
    "            votes = match.group(2)\n",
    "            return float(rating), int(votes)\n",
    "        else:\n",
    "            # print(\"Find nothing!\")\n",
    "            return None, None\n",
    "\n",
    "    def get_date(text: str) -> str:\n",
    "        match = re.search(r'Published on (\\w+ \\d{1,2}, \\d{4})', text)\n",
    "\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "            formatted_date = date_obj.strftime('%m/%d/%Y')\n",
    "            return formatted_date\n",
    "        else:\n",
    "            # print(\"Find nothing!\")\n",
    "            return None\n",
    "\n",
    "    response = requests.get(url)\n",
    "    page_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    check_post = page_soup.find('div', class_='pt-cv-no-post')\n",
    "    if check_post:\n",
    "        # print(\"No post found\")\n",
    "        return \n",
    "\n",
    "    ################################################################################\n",
    "    titles = []; contents = []; ratings = []; votes = []; authors = []; dates = []\n",
    "    stories = page_soup.find_all('h4', class_='pt-cv-title')[:12]\n",
    "    for story in stories:\n",
    "        title = story.text\n",
    "        try: \n",
    "            sub_url = story.find('a')['href']\n",
    "            sub_response = requests.get(sub_url)\n",
    "            sub_page_soup = BeautifulSoup(sub_response.content, 'html.parser')\n",
    "\n",
    "            # content fetching\n",
    "            post_text_inner = sub_page_soup.find('div', class_='post_text_inner')\n",
    "            paragraphs = post_text_inner.find_all('p')[3:-2] # 3 to -2 is the range of content\n",
    "            content = '\\n'.join([paragraph.text for paragraph in paragraphs])\n",
    "\n",
    "            rating_div = sub_page_soup.find('div', class_='gdrts-rating-text')\n",
    "            rating, vote = get_rating_and_votes(rating_div.text) \n",
    "\n",
    "            # information fetching\n",
    "            information_block = sub_page_soup.find('div', class_='code-block code-block-1')\n",
    "            author = information_block.find_all('a')[0].text\n",
    "            date = get_date(information_block.find('em').text)\n",
    "\n",
    "            # print(title, rating, vote, author, date)   \n",
    "            titles.append(title); contents.append(content); ratings.append(rating); votes.append(vote); authors.append(author); dates.append(date)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} at {title}\", sep=' ')\n",
    "\n",
    "\n",
    "    one_page_data = {\n",
    "        'titles': titles, 'contents': contents, 'ratings': ratings, 'votes': votes, 'authors': authors, 'dates': dates\n",
    "    }\n",
    "\n",
    "    return one_page_data\n",
    "\n",
    "\n",
    "final_data = {}\n",
    "is_able_to_fetch = True\n",
    "current_page = 1\n",
    "while is_able_to_fetch:\n",
    "    print(f\"Fetching data from page {current_page}.\", end=' ')\n",
    "    url = f\"https://www.creepypastastories.com/?_page={current_page}\"\n",
    "    data = fetch_one_page(url)    \n",
    "    if data:\n",
    "        for key in data:\n",
    "            if key in final_data:\n",
    "                final_data[key].extend(data[key])\n",
    "            else:\n",
    "                final_data[key] = data[key]\n",
    "        print(\"Done!\")\n",
    "        current_page += 1\n",
    "    else:\n",
    "        is_able_to_fetch = False\n",
    "        print(\"No post found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_data)\n",
    "df.head()\n",
    "\n",
    "# remove duplicate\n",
    "df = df.drop_duplicates(subset=['titles'])\n",
    "\n",
    "# save to csv\n",
    "df.to_csv('dataset.tsv', index=False, encoding='utf-8', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
