{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in d:\\gitbuh\\horror-story-generator\\gen_env\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: regex in d:\\gitbuh\\horror-story-generator\\gen_env\\lib\\site-packages (2024.11.6)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\gitbuh\\horror-story-generator\\gen_env\\lib\\site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\gitbuh\\horror-story-generator\\gen_env\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\gitbuh\\horror-story-generator\\gen_env\\lib\\site-packages (from beautifulsoup4->bs4) (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re   \n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 1. Done!\n",
      "Fetching data from page 2. Error: list index out of range at The Fortune Teller\n",
      "Error: list index out of range at The New Catacomb\n",
      "Error: list index out of range at The Terror of Blue John Gap\n",
      "Done!\n",
      "Fetching data from page 3. Done!\n",
      "Fetching data from page 4. Done!\n",
      "Fetching data from page 5. Done!\n",
      "Fetching data from page 6. Done!\n",
      "Fetching data from page 7. Done!\n",
      "Fetching data from page 8. Done!\n",
      "Fetching data from page 9. Done!\n",
      "Fetching data from page 10. Done!\n",
      "Fetching data from page 11. Done!\n",
      "Fetching data from page 12. Done!\n",
      "Fetching data from page 13. Done!\n",
      "Fetching data from page 14. Done!\n",
      "Fetching data from page 15. Done!\n",
      "Fetching data from page 16. Done!\n",
      "Fetching data from page 17. Done!\n",
      "Fetching data from page 18. Error: list index out of range at The Wrong Door\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def fetch_one_page(url: str) -> dict:\n",
    "    def get_rating_and_votes(text: str) -> tuple:\n",
    "        match = re.search(r'Rating: (\\d+\\.\\d+)/10\\. From (\\d+) vote[s]?\\.', text)\n",
    "        if match:\n",
    "            rating = match.group(1)\n",
    "            votes = match.group(2)\n",
    "            return float(rating), int(votes)\n",
    "        else:\n",
    "            # print(\"Find nothing!\")\n",
    "            return None, None\n",
    "\n",
    "    def get_date(text: str) -> str:\n",
    "        match = re.search(r'Published on (\\w+ \\d{1,2}, \\d{4})', text)\n",
    "\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "            formatted_date = date_obj.strftime('%m/%d/%Y')\n",
    "            return formatted_date\n",
    "        else:\n",
    "            # print(\"Find nothing!\")\n",
    "            return None\n",
    "\n",
    "    response = requests.get(url)\n",
    "    page_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    check_post = page_soup.find('div', class_='pt-cv-no-post')\n",
    "    if check_post:\n",
    "        # print(\"No post found\")\n",
    "        return \n",
    "\n",
    "    ################################################################################\n",
    "    titles = []; contents = []; ratings = []; votes = []; authors = []; dates = []\n",
    "    stories = page_soup.find_all('h4', class_='pt-cv-title')[:12]\n",
    "    for story in stories:\n",
    "        title = story.text\n",
    "        try: \n",
    "            sub_url = story.find('a')['href']\n",
    "            sub_response = requests.get(sub_url)\n",
    "            sub_page_soup = BeautifulSoup(sub_response.content, 'html.parser')\n",
    "\n",
    "            # content fetching\n",
    "            post_text_inner = sub_page_soup.find('div', class_='post_text_inner')\n",
    "            paragraphs = post_text_inner.find_all('p')[3:-2] # 3 to -2 is the range of content\n",
    "            content = '\\n'.join([paragraph.text for paragraph in paragraphs])\n",
    "\n",
    "            rating_div = sub_page_soup.find('div', class_='gdrts-rating-text')\n",
    "            rating, vote = get_rating_and_votes(rating_div.text) \n",
    "\n",
    "            # information fetching\n",
    "            information_block = sub_page_soup.find('div', class_='code-block code-block-1')\n",
    "            author = information_block.find_all('a')[0].text\n",
    "            date = get_date(information_block.find('em').text)\n",
    "\n",
    "            # print(title, rating, vote, author, date)   \n",
    "            titles.append(title); contents.append(content); ratings.append(rating); votes.append(vote); authors.append(author); dates.append(date)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} at {title}\", sep=' ')\n",
    "\n",
    "\n",
    "    one_page_data = {\n",
    "        'titles': titles, 'contents': contents, 'ratings': ratings, 'votes': votes, 'authors': authors, 'dates': dates\n",
    "    }\n",
    "\n",
    "    return one_page_data\n",
    "\n",
    "\n",
    "final_data = {}\n",
    "is_able_to_fetch = True\n",
    "current_page = 1\n",
    "while is_able_to_fetch and current_page <= 18:\n",
    "    print(f\"Fetching data from page {current_page}.\", end=' ')\n",
    "    url = f\"https://www.creepypastastories.com/?_orderby=date%2Cdesc&_page={current_page}\"\n",
    "    data = fetch_one_page(url)    \n",
    "    if data:\n",
    "        for key in data:\n",
    "            if key in final_data:\n",
    "                final_data[key].extend(data[key])\n",
    "            else:\n",
    "                final_data[key] = data[key]\n",
    "        print(\"Done!\")\n",
    "        current_page += 1\n",
    "    else:\n",
    "        is_able_to_fetch = False\n",
    "        print(\"No post found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m date\n\u001b[32m      2\u001b[39m date = \u001b[38;5;28mstr\u001b[39m(date.today()).replace(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m.DataFrame(final_data)\n\u001b[32m      5\u001b[39m df.head()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# remove duplicate\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "date = str(date.today()).replace('-', '_')\n",
    "\n",
    "df = pd.DataFrame(final_data)\n",
    "df.head()\n",
    "\n",
    "# remove duplicate\n",
    "df = df.drop_duplicates(subset=['titles'])\n",
    "\n",
    "# save to csv\n",
    "df.to_json(f'./creepypastastories/creepypasta_stories_{date}.json', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
